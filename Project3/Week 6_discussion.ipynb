{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommender Systems\n",
    "\n",
    "In this discussion, we introduce some basics of recommender systems (which is also covered in the lectures) in the context of movie recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content based recommendation\n",
    "<!-- Recommend items to customer x simialr to previous items rated highly by x. <- this sounds like item-item collaborative filtering -->\n",
    "\n",
    "If we know the features of items, we can exploit that information to learn the preference of users, and further infer how would a user rate an un-rated item.\n",
    "\n",
    "For example, if a user who has watched a lot of sci-fi movies and series as well as western ones, HBO is likely to recommend *Westworld* to the user.\n",
    "\n",
    "ref : https://www.youtube.com/watch?v=2uxXPzm-7FY\n",
    "\n",
    "<!--![3721518765433_.pic_hd.jpg](attachment:3721518765433_.pic_hd.jpg)-->\n",
    "![pipeline](pipeline.jpg)\n",
    "\n",
    "<br /><br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Following the notations in the project statement. There are $m$ users and $n$ movies. $r_{ij}$ is the rate from user $i$ to movie $j$. We also use a matrix $\\mathbf{W}$ to denote whether a rate entry is known:\n",
    "\n",
    "$$W_{ij} = \\begin{cases}1,& \\text{if }r_{ij}\\text{ is known}\\\\0,& \\text{if }r_{ij}\\text{ is unknown}\\end{cases}$$\n",
    "\n",
    "<!--![3711518765433_.pic_hd.jpg](attachment:3711518765433_.pic_hd.jpg)-->\n",
    "![content_based_recommender](content_based_recommender.jpg)\n",
    "\n",
    "Credit: Coursera Course: Machine Learning provided by Andrew Ng\n",
    "https://www.youtube.com/watch?v=giIXNoiqO_U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<!--![3711518765433_.pic_hd.jpg](attachment:3711518765433_.pic_hd.jpg)-->\n",
    "![content_based_recommender](content_based_recommender.jpg)\n",
    "A simple thought would be using linear regression to learn the user preferences:\n",
    "\n",
    "$$\\hat{r}_{ij} = \\mathbf{\\theta}_i^T \\mathbf{x}_j + b_i$$\n",
    "\n",
    "Here, $\\mathbf{\\theta}_i \\in \\mathbb{R}^K$ is the preference of the $i$th user.\n",
    "\n",
    "If we let $\\widetilde{\\mathbf{\\theta}}_i = [b_i, \\mathbf{\\theta}_i]^T \\in \\mathbb{R}^{K+1}$, $\\widetilde{\\mathbf{x}}_j = [1, \\mathbf{x}_j]^T \\in \\mathbb{R}^{K+1}$, then the equation becomes\n",
    "$$\\hat{r}_{ij} = \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j$$\n",
    "\n",
    "Now to learn the preferences - $\\widetilde{\\mathbf{\\theta}}_i$'s, we can optimize the following problem:\n",
    "$$\\mathop{\\rm minimize}_\\limits{\\widetilde{\\mathbf{\\theta}}_1,\\dots,\\widetilde{\\mathbf{\\theta}}_m} \\sum_{i,j: W_{ij}=1} \\lVert r_{ij} - \\hat{r}_{ij}\\rVert^2 = \\sum_{i,j: W_{ij}=1} (r_{ij} - \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\mathop{\\rm minimize}_\\limits{\\widetilde{\\mathbf{\\theta}}_1,\\dots,\\widetilde{\\mathbf{\\theta}}_m} \\sum_{i,j: W_{ij}=1} \\lVert r_{ij} - \\hat{r}_{ij}\\rVert^2 = \\sum_{i,j: W_{ij}=1} (r_{ij} - \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j)^2$$\n",
    "We can also add regularization terms:\n",
    "$$\\mathop{\\rm minimize}_\\limits{\\widetilde{\\mathbf{\\theta}}_1,\\dots,\\widetilde{\\mathbf{\\theta}}_m} \\sum_{i,j: W_{ij}=1} \\lVert r_{ij} - \\hat{r}_{ij}\\rVert^2 + \\lambda \\sum_{i=1}^m \\lVert\\widetilde{\\mathbf{\\theta}}_i\\rVert^2= \\sum_{i,j: W_{ij}=1} (r_{ij} - \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j)^2 + \\lambda \\sum_{i=1}^m\\sum_{j=1}^{K+1}\\widetilde{\\theta}_{ij}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "<!--![3701518765433_.pic.jpg](attachment:3701518765433_.pic.jpg)-->\n",
    "![collaborative_filtering](collaborative_filtering.png)\n",
    "\n",
    "However, extracting (good) features can be hard.\n",
    "\n",
    "Collaborative filtering uses the similarity between users/items to make recommendations.\n",
    "\n",
    "The intuition is that, for example, one of your friend likes the movies you like, and also gives low ratings to the movies that you dislike. Now one day your friend watches a new movie (which you haven't watched), and your friend really likes it, it is very likely that you will like it too. This is called **user-user collaborative filtering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "![collaborative_filtering](collaborative_filtering.png)\n",
    "\n",
    "For another example, two movies are often simultaneously rated high/low (a na√Øve example would be sequals). Then when you rate one of them high, you'll probably like the other one as well. This is called **item-item collaborative filtering**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define similarity\n",
    "\n",
    "1. Jaccard similarity (commonly used for sets)\n",
    "2. Cosine similarity\n",
    "  $$\\mathop{\\rm CosSim}(\\mathbf{x}, \\mathbf{y})=\\frac{<\\mathbf{x}, \\mathbf{y}>}{\\lVert\\mathbf{x}\\rVert \\cdot \\lVert\\mathbf{y}\\rVert}$$\n",
    "  Problem: not invariant to shifts. $\\mathop{\\rm CosSim}(\\mathbf{x}, \\mathbf{y}) \\neq \\mathop{\\rm CosSim}(\\mathbf{x}+\\mathbf{1}, \\mathbf{y})$\n",
    "3. Centered cosine (Pearson-correlation coecient)\n",
    "  $$\\mathop{\\rm Pearson}(\\mathbf{x}, \\mathbf{y})=\\frac{<\\mathbf{x}-\\bar{\\mathbf{x}}, \\mathbf{y}-\\bar{\\mathbf{y}}>}{\\lVert\\mathbf{x}-\\bar{\\mathbf{x}}\\rVert \\cdot \\lVert\\mathbf{y}-\\bar{\\mathbf{y}}\\rVert}$$\n",
    "\n",
    "ref: https://zh.coursera.org/learn/machine-learning/lecture/2WoBV/collaborative-filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### User-user collaborative filtering\n",
    "\n",
    "Now that we can find the similar users, how do we predict the ratings?\n",
    "\n",
    "#### k-Nearest Neighbor (KNN)\n",
    "\n",
    "To predict $r_{ij}$, find the top $k$ similar users of user $i$ who have rated the movie $j$ (let this set of $k$ users be $P_i$), and use the average of their ratings. That is\n",
    "\n",
    "$$\\hat{r}_{ij} = \\frac{\\sum_{v \\in P_i} r_{vj}}{k}$$\n",
    "\n",
    "Considering that each user may have their own baseline of ratings, we may modify this to\n",
    "\n",
    "$$\\hat{r}_{ij} - \\mu_i = \\frac{\\sum_{v \\in P_i} r_{vj} - \\mu_v}{k}$$\n",
    "\n",
    "Further, if we give more weights to more similar users, the equation turns into\n",
    "$$\\hat{r}_{ij} - \\mu_i = \\frac{\\sum_{v \\in P_i} \\mathrm{sim}_{iv}(r_{vj} - \\mu_v)}{\\sum_{v \\in P_i} \\mathrm{sim}_{iv}}$$\n",
    "This is the equation you are asked to use in Project 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenges\n",
    "\n",
    "- Sparsity: Sparse rating matrices limits the coverage of neighborhood-based methods. It creates challenges for robust similarity computation when the number of mutually rated items between two users is small\n",
    "- Scalability: Onine phase of neighborhood-based methods can sometimes be impractical in large-scale settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Factorization\n",
    "\n",
    "Recall what we had in content-based recommender systems:\n",
    "$$\\mathop{\\rm minimize}_\\limits{\\widetilde{\\mathbf{\\theta}}_1,\\dots,\\widetilde{\\mathbf{\\theta}}_m} \\sum_{i,j: W_{ij}=1} (r_{ij} - \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j)^2 + \\lambda \\sum_{i=1}^m\\sum_{j=1}^{K+1}\\widetilde{\\theta}_{ij}^2$$\n",
    "\n",
    "What if we have access to the user preferences but not movie features?\n",
    "\n",
    "$$\\mathop{\\rm minimize}_\\limits{\\widetilde{\\mathbf{x}}_1,\\dots,\\widetilde{\\mathbf{x}}_m} \\sum_{i,j: W_{ij}=1} (r_{ij} - \\widetilde{\\mathbf{\\theta}}_i^T \\widetilde{\\mathbf{x}}_j)^2 + \\lambda \\sum_{i=1}^n\\sum_{j=1}^{K+1}\\widetilde{x}_{ij}^2$$\n",
    "\n",
    "So we can not only learn the preferences in this way; we can also learn features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Matrix Factorization\n",
    "\n",
    "By alternatively optimizing $\\sum_{i,j: W_{ij}=1} \\lVert r_{ij} - \\hat{r}_{ij}\\rVert^2$ (with regularization terms) over $\\widetilde{\\mathbf{x}}_i$'s and $\\widetilde{\\mathbf{\\theta}}_i$'s, we can actually learn both user preferences and item features.\n",
    "\n",
    "Actually, we can combine the two optimization problems together, and get\n",
    "\n",
    "$$\\mathop{\\rm minimize}_{\\substack{\\mathbf{\\theta}_1,\\dots,\\mathbf{\\theta}_m \\\\ \\mathbf{x}_1,\\dots,\\mathbf{x}_n}} \\sum_{i,j: W_{ij}=1} (r_{ij} - \\mathbf{\\theta}_i^T \\mathbf{x}_j)^2 + \\lambda \\sum_{i=1}^n \\lVert x_{i}\\rVert^2 + \\lambda \\sum_{i=1}^m\\lVert\\theta_{i}\\rVert^2$$\n",
    "\n",
    "This can be rewritten to a weighted matrix factorization problem:\n",
    "\n",
    "$$\\mathop{\\rm minimize}_{\\mathbf{X},\\mathbf{\\Theta}} \\sum_{i,j} W_{ij}(r_{ij} - (\\mathbf{\\Theta} \\mathbf{X})_{ij})^2 + \\lambda \\lVert \\mathbf{X}\\rVert_F^2 + \\lambda \\lVert \\mathbf{\\Theta}\\rVert_F^2$$\n",
    "\n",
    "$$\\mathbf{\\Theta} = \\begin{bmatrix}\\mathbf{\\theta}_1^T \\\\ \\vdots \\\\ \\mathbf{\\theta}_m^T\\end{bmatrix}, \\qquad\n",
    "\\mathbf{X} = \\begin{bmatrix}\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\end{bmatrix}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
